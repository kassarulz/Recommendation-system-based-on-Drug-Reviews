{"cells":[{"metadata":{"_uuid":"a6013e8a57b2f9043ab25e274e565868a00f76cd"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"b9ffd57345189133401fa11378be7dd5224e10ae"},"cell_type":"markdown","source":"**Team**: 5 top 100\n\n**Univesity**: PFUR/RUDN \n\nThis kernel was made by:\n\n[Stanislav Prikhodko](http://www.kaggle.com/stasian)\n    \n[Daniil Larionov](http://kaggle.com/rexhaif)\n\n[Rustem Zalyalov](http://www.kaggle.com/kaichou)\n\n[Katherine Lozhenko](http://www.kaggle.com/notkappa)\n\n[Sergey Kuzmin](http://www.kaggle.com/seekuu)"},{"metadata":{"_uuid":"544dae70888022a55ba3e1addf56f605024720db"},"cell_type":"markdown","source":"**Introduction**\nThis kernel was made to formalize reviews for the prescripted medicine and considers how such reviews might develop in the future. Medication feedback made by customers can help them find the most cost-effectiveness drug and for pharmacists there is good evidence that medication review improves process outcomes of prescribing and spot out specific problems early on. \nWe will use MAE for this because we don't need positive errors to cancel out negative ones or to give a relatively high weight to large errors."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import os, pickle\nimport re, gc\nimport keras\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom keras.preprocessing import text, sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f780ff1ddad972203585b35104b3dc98e550c03a"},"cell_type":"markdown","source":"Letâ€™s prepare the data.\n\nTip: you can use \"config style\" for code reusability and faster experiments"},{"metadata":{"trusted":true,"_uuid":"c15735ced18a30224e390cdabc5ec6ac085b1043"},"cell_type":"code","source":"conf = {\n    \"kaggle\" : True,\n    \"embedding_file\" : '../input/glove840b300dtxt/glove.840B.300d.txt',\n    \"embedding_pickle\" : '/data/glove.840B.300d/glove.840B.300d.pickle',\n    \"train_path\" : \"../input/kuc-hackathon-winter-2018/drugsComTrain_raw.csv\",\n    \"test_path\" : \"../input/kuc-hackathon-winter-2018/drugsComTest_raw.csv\",\n    \n    \n    \"max_features\" : 30000,\n    \"maxlen\" : 100,\n    \"embed_size\" : 300,\n    \n    \n    \"batch_size\":3000,\n    \"epochs\":500,\n    \"n_splits\":10,\n    \"random_state\":0\n}\n\nmodel_conf = {\n    \"SpDr\": 0.2,\n    \"GRU_Units\":128,\n    \"Conv_Units\":128,\n    \"conv_kernel\":1,\n    \"Dense1_Unit\":128,\n    \"Dr1\":0.2,\n    \"Dense2_Unit\":128,\n    \"Dr2\": 0.2,\n    \n    \n    \"lr\" : 1e-4,\n    \"loss\" : 'mean_absolute_error',\n    \"metrics\": ['mae']#list\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77e6cce6b2bbcbfa7db6df2aa7cfcc0118aca528"},"cell_type":"markdown","source":"The first thing we must check is the distribution of rating.This graph is bimodal distribution due to the reasons why customers write review in the first place."},{"metadata":{"trusted":true,"_uuid":"3f6b4bd296a25b1d5bf223e0427a8cd3113a5530"},"cell_type":"code","source":"d_train = pd.read_csv(conf[\"train_path\"])\nd_test = pd.read_csv(conf[\"test_path\"])\n\nplt.figure(figsize=(8,8))\nsns.distplot(d_train['rating'])\n\nplt.xlabel('Rating')\nplt.ylabel('Dist')\nplt.title(\"Distribution of rating\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f9552db3ce83d8f9d5418689e6c1e97b1872aaf"},"cell_type":"markdown","source":"We examine the total amount of rewiews made per year. Growth from year 2015 and later have reasons (maybe social) unimportant for the current goal"},{"metadata":{"trusted":true,"_uuid":"ee7c22f026b0d6ac28f0746e62f18ec5f204bf62"},"cell_type":"code","source":"d_train['year'] = pd.to_datetime(d_train['date'], errors='coerce')\ncnt = d_train['year'].dt.year.value_counts()\ncnt = cnt.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(cnt.index, cnt.values,color='blue',alpha=0.4)\nplt.xticks(rotation='vertical')\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews per year\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a129ff9aaa9ee25b3512d83db54a0c74bfc6859a"},"cell_type":"markdown","source":"We examine the total amount of rewiews made per month. The graph is constant so it isn't really important."},{"metadata":{"trusted":true,"_uuid":"1004c7d501e835abfb2af0f880cd6c68039c6598"},"cell_type":"code","source":"d_train['month'] = pd.to_datetime(d_train['date'], errors='coerce')\ncnt = d_train['month'].dt.month.value_counts()\ncnt = cnt.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(cnt.index, cnt.values,color='blue',alpha=0.4)\nplt.xticks(rotation='vertical')\nplt.xlabel('Month', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews per month\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b49410c628f1b6d76ba60870d480ff015c5877fe"},"cell_type":"markdown","source":"We examine the total amount of rewiews made per day. As the previous one, the graph is constant ."},{"metadata":{"trusted":true,"_uuid":"76208a9b2e715632a66ad9d23a088e350bc23451"},"cell_type":"code","source":"d_train['day'] = pd.to_datetime(d_train['date'], errors='coerce')\ncnt = d_train['day'].dt.day.value_counts()\ncnt = cnt.sort_index()\nplt.figure(figsize=(9,6))\nsns.barplot(cnt.index, cnt.values,color='blue',alpha=0.4)\nplt.xticks(rotation='vertical')\nplt.xlabel('Day', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Reviews per day\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ff3caf97d124cec78d0d6a2cd551a917c493e6a"},"cell_type":"markdown","source":"Top 10 popular drugs. Birth control and antidepressants are the most common drugs to see on top"},{"metadata":{"trusted":true,"_uuid":"c1625312ec2122dc19a8614a1ab2748906a63f44"},"cell_type":"code","source":"drug = d_train['drugName'].value_counts()\n\nplt.figure(figsize=(20,6))\n\nsns.barplot(drug[:15].index, drug[:15],color='blue',alpha=0.4)\n\nplt.xlabel('Name of drug', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title(\"Top 10 popular drugs\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e46556e3ce7a21bd3c1ce98033f3cad3f7a07fd"},"cell_type":"markdown","source":"Number of users who found review useful. Gamma distribution it is"},{"metadata":{"trusted":true,"_uuid":"ce91734f33e108a831311a46dbb0e88a7ac8f934"},"cell_type":"code","source":"plt.figure(figsize=(16,8))\n\nsns.distplot(d_train['usefulCount'].value_counts())\n\n\nplt.xlabel('Number of users who found review useful')\nplt.ylabel('Dist')\nplt.title(\"Distribution of usefulCount\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f4418fd5b4c840cdbddc9ef98b96749c888ecf6"},"cell_type":"markdown","source":"Correlation between rating and users who found review useful. Positive linear correlation\n\nAcording to this we decided to use **only** text data for preventing major **data leak** and, you know, be more usefull in real world application"},{"metadata":{"trusted":true,"_uuid":"a690e358203a0d317e69ca620a042da3223d7f36"},"cell_type":"code","source":"plt.figure(figsize=(8,6))\n\nsns.lineplot(x='rating',y='usefulCount',data=d_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5eb953bbb06b6c0a0887adeea1504326110f867d"},"cell_type":"markdown","source":"Test the distribution of ratings in train and test dataset. Graphs are similar, the data was choosen correctly "},{"metadata":{"trusted":true,"_uuid":"35d9efee1af1f16d412fdb7597bbd832133bcde6"},"cell_type":"code","source":"plt.figure(figsize=(9,9))\n\nsns.distplot(d_train['rating'])\nsns.distplot(d_test['rating'],color='violet')\n\nplt.xlabel('Rating')\nplt.ylabel('Dist')\nplt.title(\"Distribution of rating (train and test)\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4613b408dab3e42c37b3b222b8920d04ab9616c"},"cell_type":"markdown","source":"For easier predictions we divide ratings by 10"},{"metadata":{"trusted":true,"_uuid":"bff077dc2294c3ffd6d8e8c63c50c56468438c36"},"cell_type":"code","source":"train = pd.read_csv(conf[\"train_path\"])\ntest = pd.read_csv(conf[\"test_path\"])\n\nX_train = train[\"review\"]\ny_train = train[\"rating\"].values / 10\n\nX_test = test[\"review\"]\nsample_pred = np.zeros_like(test[\"rating\"], dtype=np.float32)\n\ntokenizer = text.Tokenizer(num_words=conf[\"max_features\"])\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=conf[\"maxlen\"])\nx_test = sequence.pad_sequences(X_test, maxlen=conf[\"maxlen\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0178c0cc89b0cba736537af9d34ba400af4cd550"},"cell_type":"markdown","source":"Split reviews by words  and do embedding matrix\n\nWe choose Glove embedding, cause we earned best scores here"},{"metadata":{"trusted":true,"_uuid":"b3a87eb30448c4e0a506ea35a7f0fa039c2f738a"},"cell_type":"code","source":"if conf[\"kaggle\"]: #tip: cause of config code style you can use same code of local experiments & for kaggle submission\n    def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(conf[\"embedding_file\"],encoding=\"UTF-8\"))\nelse:\n    with open(conf[\"embedding_pickle\"], 'rb') as handle: #if you dont want to wait 6-9 minuts you can pickle your dict once\n        embeddings_index = pickle.load(handle) # and just unpickle next, it may take near 10-20 seconds\n    \nword_index = tokenizer.word_index\nnb_words = min(conf[\"max_features\"], len(word_index))\nembedding_matrix = np.zeros((nb_words, conf[\"embed_size\"]))\nfor word, i in word_index.items():\n    if i >= conf[\"max_features\"]: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \nconf[\"embedding_matrix\"] = embedding_matrix\n\ndel embeddings_index\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98cf76021eec5055d0f7cf97a84e5f36cbd82718"},"cell_type":"markdown","source":"Writing attention mechanism.\n\nBecause word processing task can be described as sequence processing task we make two layered lstm with Attention to prevent overfitting and data augmentation\n\nActually stolen from: https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043"},{"metadata":{"trusted":true,"_uuid":"a5b3e9bd5af07de118f803ce51cd7aab18cb0734"},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    \n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcde53daadee4d0b6641d013ee8217daec3ca6ad"},"cell_type":"markdown","source":"We chose to combine 2 kinds of models: \n\nFirst is really complicated 2 layer LSTM model with Attention layer for increasing information \"capability\", its model focusing on catching sequence information. Also checkout\n\nthis: https://arxiv.org/pdf/1709.00893v1.pdf\n\nand this: https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043  we changed alot, but base arcitecture was took"},{"metadata":{"trusted":true,"_uuid":"862951ce7e81e9a9ed42752262b46651b89db43b"},"cell_type":"code","source":"def model_lstm_atten(main_conf: dict, model_conf:dict):\n    inp = Input(shape=(main_conf[\"maxlen\"], ))\n    x = Embedding(main_conf[\"max_features\"], main_conf[\"embed_size\"], weights=[main_conf[\"embedding_matrix\"]],trainable = False)(inp)\n    x = Bidirectional(CuDNNLSTM(model_conf[\"LSTM_first_layer_units\"], return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(model_conf[\"LSTM_second_layer_units\"], return_sequences=True))(x)\n    x = Attention(main_conf[\"maxlen\"])(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = Dense(1, activation=\"relu\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss=model_conf[\"loss\"],optimizer=Adam(lr=model_conf[\"lr\"]),metrics=model_conf[\"metrics\"])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46b12f6f12974b68206e107791005ceae5a1c5c3"},"cell_type":"markdown","source":"Second is old but still attrective GRU->CNN->Dense, was used by us in Toxic Classification Challange and it gave us bronze medal:)\n\nThis model is more about catchaning structure of sentences. Also checkout this:\n\nI havent find original paper(cause it wasnt on arxiv(facepalm)) but you can look here: https://arxiv.org/pdf/1806.11316v1.pdf"},{"metadata":{"trusted":true,"_uuid":"0e9915f16c491c71985aba4df2b09eb279ae196b"},"cell_type":"code","source":"def get_toxic_model(main_conf: dict, model_conf:dict):\n    sequence_input = Input(shape=(main_conf[\"maxlen\"], ))\n    x = Embedding(main_conf[\"max_features\"], main_conf[\"embed_size\"], weights=[main_conf[\"embedding_matrix\"]],trainable = False)(sequence_input)\n    x = SpatialDropout1D(model_conf[\"SpDr\"])(x)\n    x = Bidirectional(CuDNNGRU(model_conf[\"GRU_Units\"], return_sequences=True))(x)\n    x = Conv1D(model_conf[\"Conv_Units\"], kernel_size = model_conf[\"conv_kernel\"], padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    x = concatenate([avg_pool, max_pool])\n    x = Dense(model_conf[\"Dense1_Unit\"], activation='relu')(x)\n    x = Dropout(model_conf[\"Dr1\"])(x)\n    x = Dense(model_conf[\"Dense2_Unit\"], activation='relu')(x)\n    x = Dropout(model_conf[\"Dr2\"])(x)\n    preds = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(sequence_input, preds)\n    model.compile(loss=model_conf[\"loss\"],optimizer=Adam(lr=model_conf[\"lr\"]),metrics=model_conf[\"metrics\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acaae8466a9aa6a714ea32b9d71cff2dbef39dd1"},"cell_type":"markdown","source":"Using out of fold predictions and  blending them"},{"metadata":{"trusted":true,"_uuid":"f4f7e20d9628cfa9c01ae56bd3740e1965583f51"},"cell_type":"code","source":"kf = KFold(n_splits=conf[\"n_splits\"], shuffle=True, random_state=conf[\"random_state\"])\ncnt = 0\nmodel_conf[\"LSTM_first_layer_units\"] = 128\nmodel_conf[\"LSTM_second_layer_units\"] = 64\n\nfor train_index, test_index in kf.split(x_train, y_train):\n    model = model_lstm_atten(conf, model_conf)\n    es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n\n    hist = model.fit(x_train[train_index], y_train[train_index], batch_size=conf[\"batch_size\"], epochs=conf[\"epochs\"], \n                     validation_data=(x_train[test_index], y_train[test_index]),callbacks=[es])\n    model.save(f\"model_lstm_atten_cv_{cnt}.keras\")\n    \n    sample_pred +=  model.predict(x_test, conf[\"batch_size\"]*2).reshape(-1)\n    cnt+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59908c2870679b057aad8aa27c43ead3559f3d91"},"cell_type":"code","source":"kf = KFold(n_splits=conf[\"n_splits\"], shuffle=True, random_state=conf[\"random_state\"])\ncnt = 0\n\nfor train_index, test_index in kf.split(x_train, y_train):\n    model = get_toxic_model(conf, model_conf)\n    es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n\n    hist = model.fit(x_train[train_index], y_train[train_index], batch_size=conf[\"batch_size\"], epochs=conf[\"epochs\"], \n                     validation_data=(x_train[test_index], y_train[test_index]),callbacks=[es])\n    model.save(f\"model_gru_cnn_{cnt}.keras\")\n    \n    sample_pred +=  model.predict(x_test, conf[\"batch_size\"]*2).reshape(-1)\n    cnt+=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b34e64e4c59317507134aad418d2a996c570bc2b"},"cell_type":"code","source":"print(mean_absolute_error(test[\"rating\"], 10 * sample_pred/ conf[\"n_splits\"]/2))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}